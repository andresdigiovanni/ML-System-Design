# **4. Defining Metrics**

Defining the right metrics is essential for guiding model development, comparing alternatives, and monitoring performance post-deployment. Good metrics reflect both technical performance and business impact.

---

## **4.1 Offline Metrics (Model Evaluation)**

These metrics are used during training and validation to evaluate how well the model performs on historical data. The choice depends on the ML task:

### ✅ **Classification**
- **Accuracy**: Proportion of correct predictions.
- **Precision / Recall / F1 Score**: Useful when classes are imbalanced.
- **ROC-AUC**: Measures ability to rank positives above negatives.
- **Log Loss**: Penalizes wrong predictions with high confidence.

### ✅ **Regression**
- **Mean Squared Error (MSE)**: Heavily penalizes large errors.
- **Mean Absolute Error (MAE)**: Less sensitive to outliers.
- **R² Score**: Proportion of variance explained.

### ✅ **Ranking / Recommendation**
- **NDCG** (Normalized Discounted Cumulative Gain)
- **MAP** (Mean Average Precision)
- **Hit Rate / Top-K Accuracy**

### ✅ **Time Series / Forecasting**
- **MAE / RMSE per time step**
- **Mean Absolute Percentage Error (MAPE)**
- **Coverage**: Percentage of true values inside prediction intervals.

> *Offline metrics help guide experimentation and hyperparameter tuning, but they do not always reflect real-world utility.*

---

## **4.2 Online Metrics (Production Performance)**

Once deployed, the system’s real-world performance must be measured via **live, user-facing metrics**, such as:

- **Click-through rate (CTR)**
- **Conversion rate**
- **Churn reduction**
- **Revenue impact**
- **Task completion rate**
- **Latency and throughput**

These metrics are usually tracked via A/B tests, shadow deployments, or canary releases.

---

## **4.3 Non-Functional Metrics**

Beyond predictive performance, your system should also be evaluated on **engineering and operational aspects**:

| Metric                      | Why It Matters                                      |
|----------------------------|-----------------------------------------------------|
| Training time               | Impacts iteration speed and scalability             |
| Inference latency           | Critical for real-time applications                 |
| Resource usage (CPU/GPU)    | Affects cost and deployment feasibility             |
| Model size / complexity     | Important for edge/mobile deployments               |
| Debuggability               | Ease of explaining predictions and troubleshooting  |
| Extensibility               | Ability to add new features or retrain in future    |

> *You don’t want a model that’s 1% more accurate but 10x harder to maintain.*

---

## **4.4 Metric Design Best Practices**

- Use **multiple metrics** to get a holistic view (e.g., AUC + F1 + latency).
- Define a **primary metric** to optimize during training.
- Set **guardrails** to prevent regressions (e.g., no drop in recall > 5%).
- Choose **thresholds** for classification carefully, especially with imbalanced classes.
- Align metrics with **business KPIs** to ensure model relevance.

---

## **4.5 Metric Evolution**

Over time, your system may need to adapt to:

- **New data distributions**
- **Evolving product goals**
- **User behavior changes**

Revisiting your metric design periodically is key to staying aligned with real-world impact.
